{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb5106c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 라이브러리\n",
        "\n",
        "# !pip install -q \"transformers>=4.42.0\" \"accelerate>=0.30.0\" bitsandbytes peft datasets pillow mlflow\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import mlflow\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from transformers.integrations import MLflowCallback\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"torch\", torch.__version__, \"cuda\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1038f00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 데이터 로드 (synthetic_final.ndjson)\n",
        "data_path = Path(\"../mixture_data/synthetic_final.ndjson\")\n",
        "rows = [json.loads(l) for l in data_path.open(encoding=\"utf-8\") if l.strip()]\n",
        "print(\"rows\", len(rows), rows[0].keys())\n",
        "\n",
        "full_ds = Dataset.from_list(rows)\n",
        "splits = full_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset, eval_dataset = splits[\"train\"], splits[\"test\"]\n",
        "print(\"train:\", len(train_dataset), \"eval:\", len(eval_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8faa9330",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) 모델/프로세서 로드 (4bit)\n",
        "base_model = \"Qwen/Qwen3-VL-7B-Instruct\"  # VRAM에 맞게 조정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcdf885",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) LoRA 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a606dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) 프롬프트/콜레이터\n",
        "def build_messages(r, img):\n",
        "    sys_prompt = (\n",
        "        \"너는 얼굴 사진과 진단명을 받아 1) 진단을 보조/검증하고 2) 주어진 건강정보/칼로리/룰에 맞춰 \"\n",
        "        \"식단 JSON만 생성하는 VLM이다. JSON 외 텍스트는 답하지 말 것.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"진단: {r['diagnosis_name']}\\n\"\n",
        "        f\"키/몸무게/활동/목표: {r['height_cm']}cm, {r['weight_kg']}kg, {r['activity_level']}, {r['goal_type']}\\n\"\n",
        "        f\"1식 칼로리 목표: {r['calorie_plan']} kcal\\n\"\n",
        "        f\"식이 룰: {r['rules_text']}\\n\"\n",
        "        \"얼굴 사진을 참고해 진단을 보조/검증하되, 최종 식단 JSON만 반환.\"\n",
        "    )\n",
        "    assistant = json.dumps(r[\"diet_json\"], ensure_ascii=False)\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}, {\"type\": \"text\", \"text\": user_prompt}]},\n",
        "        {\"role\": \"assistant\", \"content\": assistant},\n",
        "    ]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, messages = [], []\n",
        "    for r in batch:\n",
        "        img_path = Path(r[\"image\"])\n",
        "        if not img_path.is_absolute():\n",
        "            img_path = (data_path.parent / img_path).resolve()\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        images.append(img)\n",
        "        messages.append(build_messages(r, img))\n",
        "\n",
        "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
        "    text_inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "    labels = text_inputs[\"input_ids\"].clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    return {\n",
        "        \"input_ids\": text_inputs[\"input_ids\"],\n",
        "        \"attention_mask\": text_inputs[\"attention_mask\"],\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": labels,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ec1999",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Trainer + MLflow\n",
        "mlflow.set_experiment(\"qwen3vl-qlora\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qlora-qwen3vl\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"mlflow\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    callbacks=[MLflowCallback()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4ac6e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) 학습 + 로깅\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"lora_r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"base_model\": base_model,\n",
        "    })\n",
        "    trainer.train()\n",
        "    save_dir = \"qlora-adapter\"\n",
        "    trainer.save_model(save_dir)\n",
        "    mlflow.log_artifacts(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bf6577",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) 추론 어댑터 로드\n",
        "from peft import PeftModel\n",
        "base_infer = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "peft_infer = PeftModel.from_pretrained(base_infer, \"qlora-adapter\")\n",
        "peft_infer.eval()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
