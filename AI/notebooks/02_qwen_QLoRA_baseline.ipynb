{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "98bb161e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers: 4.57.3\n",
            "torch: 2.3.1+cu121 cuda: True\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import mlflow\n",
        "from PIL import Image\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForVision2Seq,\n",
        "    BitsAndBytesConfig,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from transformers.integrations import MLflowCallback\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1038f00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: Counter({'아토피': 90, '정상': 90, '지루': 90, '건선': 90, '여드름': 90, '주사': 90})\n",
            "eval : Counter({'아토피': 10, '지루': 10, '여드름': 10, '정상': 10, '주사': 10, '건선': 10})\n"
          ]
        }
      ],
      "source": [
        "# 1) 데이터 로드 (synthetic_final.ndjson)->계층화샘플링\n",
        "data_path = Path(\"../mixture_data/synthetic_final.ndjson\")\n",
        "rows = [json.loads(l) for l in data_path.open(encoding=\"utf-8\") if l.strip()]\n",
        "\n",
        "train_rows, eval_rows = train_test_split(\n",
        "    rows,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=[r[\"diagnosis_name\"] for r in rows],\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_list(train_rows)\n",
        "eval_dataset = Dataset.from_list(eval_rows)\n",
        "\n",
        "print(\"train:\", Counter(train_dataset[\"diagnosis_name\"]))\n",
        "print(\"eval :\", Counter(eval_dataset[\"diagnosis_name\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4f418c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# splits = full_ds.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"diagnosis_name\")\n",
        "# train_dataset, eval_dataset = splits[\"train\"], splits[\"test\"]\n",
        "# print(\"train:\", Counter(train_dataset[\"diagnosis_name\"]))\n",
        "# print(\"eval:\", Counter(eval_dataset[\"diagnosis_name\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8faa9330",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/venv/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.50s/it]\n"
          ]
        }
      ],
      "source": [
        "# 2) 모델/프로세서 로드 (4bit) + 이미지 토큰 등록\n",
        "base_model = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0dcdf885",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 43,646,976 || all params: 8,810,770,672 || trainable%: 0.4954\n"
          ]
        }
      ],
      "source": [
        "# 3) LoRA 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2a606dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) 프롬프트/콜레이터\n",
        "def build_messages(r):\n",
        "    sys_prompt = \"너는 얼굴 이미지를 참고하여 식단 JSON만 생성하는 모델이다. JSON 외 텍스트는 출력하지 마라.\"\n",
        "\n",
        "    user_text = (\n",
        "        f\"진단: {r['diagnosis_name']}\\n\"\n",
        "        f\"키/몸무게/활동/목표: {r['height_cm']}cm, {r['weight_kg']}kg, {r['activity_level']}, {r['goal_type']}\\n\"\n",
        "        f\"1식 칼로리 목표: {r['calorie_plan']} kcal\\n\"\n",
        "        f\"식이 룰: {r['rules_text']}\\n\"\n",
        "        \"반드시 JSON만 출력하라.\"\n",
        "    )\n",
        "\n",
        "    assistant = json.dumps(r[\"diet_json\"], ensure_ascii=False)\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_text}]},\n",
        "        {\"role\": \"assistant\", \"content\": assistant},\n",
        "    ]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    messages = []\n",
        "\n",
        "    for r in batch:\n",
        "        img_path = (data_path.parent / r[\"image\"].replace(\"\\\\\", \"/\")).resolve()\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        images.append(img)\n",
        "        messages.append(build_messages(r))\n",
        "\n",
        "    # 1) chat template -> \"문자열\"로 만든다 (여기가 핵심)\n",
        "    texts = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=False,\n",
        "        tokenize=False,                # 변경: tokenize=False 로 문자열 생성\n",
        "    )\n",
        "\n",
        "    # 2) processor를 \"text+images\"로 1번만 호출 (여기가 핵심)\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    labels = inputs[\"input_ids\"].clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    batch_out = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"pixel_values\": inputs[\"pixel_values\"],\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "    # image_grid_thw 있으면 반드시 포함\n",
        "    if \"image_grid_thw\" in inputs:\n",
        "        batch_out[\"image_grid_thw\"] = inputs[\"image_grid_thw\"]\n",
        "\n",
        "    return batch_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "583909c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keys: dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels', 'image_grid_thw'])\n",
            "input_ids: (2, 1377)\n",
            "pixel_values: (8192, 1536)\n",
            "image_grid_thw: (2, 3)\n",
            "<|im_start|>system\n",
            "너는 얼굴 이미지를 참고하여 식단 JSON만 생성하는 모델이다. JSON 외 텍스트는 출력하지 마라.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|\n"
          ]
        }
      ],
      "source": [
        "# collate_fn 출력과 tokenizer decode가 정상인지 확인\n",
        "b = collate_fn([train_dataset[0], train_dataset[1]])\n",
        "print(\"keys:\", b.keys())\n",
        "print(\"input_ids:\", tuple(b[\"input_ids\"].shape))\n",
        "print(\"pixel_values:\", tuple(b[\"pixel_values\"].shape))\n",
        "if \"image_grid_thw\" in b:\n",
        "    print(\"image_grid_thw:\", tuple(b[\"image_grid_thw\"].shape))\n",
        "\n",
        "decoded = processor.tokenizer.decode(b[\"input_ids\"][0], skip_special_tokens=False)\n",
        "print(decoded[:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "36ec1999",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "You are adding a <class 'transformers.integrations.integration_utils.MLflowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "MLflowCallback\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/13 05:28:27 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 0f4038d34a2c4f87bc0a50fce6aedeaf: Failed to log run data: Exception: Changing param values is not allowed. Param with key='fp16_backend' was already logged with value='' for run ID='0f4038d34a2c4f87bc0a50fce6aedeaf'. Attempted logging new value 'auto'.\n",
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [135/135 09:26, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>18.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.333600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>8.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.412400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>6.747700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>6.424100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>6.298000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.171800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>6.083300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>6.061500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>6.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>6.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>5.996600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>6.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>5.982300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>5.986400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>5.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>5.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>5.934000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>5.936900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>5.959900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>5.941600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>5.937200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>5.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>5.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>5.933500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/root/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 5) Trainer + MLflow\n",
        "mlflow.set_experiment(\"qwen3vl-qlora\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./qlora-qwen3vl\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"mlflow\",\n",
        "    remove_unused_columns=False,\n",
        "    predict_with_generate=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    callbacks=[MLflowCallback()],\n",
        ")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"lora_r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"base_model\": base_model,\n",
        "    })\n",
        "    trainer.train()\n",
        "\n",
        "save_dir = \"qlora-adapter\"\n",
        "trainer.save_model(save_dir)\n",
        "mlflow.log_artifacts(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bf6577",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) 추론 어댑터 로드\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForVision2Seq.from_pretrained(\n",
        "    \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"qlora-adapter\")\n",
        "model.eval()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
